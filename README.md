# ğŸ“Š Narrative Analysis MVP
A lightweight system for extracting **overall summaries**, **dominant narratives**, and **representative comments** from large batches of Facebook comments using a **local Qwen2.5-7B model**.

This MVP includes:

- ğŸš€ Streamlit user interface  
- ğŸ§  Local LLM backend (Qwen2.5-7B-Instruct)  
- ğŸ§¹ A JSON restructuring tool for normalization  
- ğŸ“¦ Modular backend analysis engine  
- ğŸ“ Support for JSON uploads or manual text input  

# How to run ğŸš€

- Pull analysis.py restructure.py ui.py
- Run restructure-emoji.py --> prompts for JSON selection --> select raw JSON file from desktop --> saves structured JSON on desktop
- Run [streamlit run ui.py] --> opens streamlit ui --> upload structured JSON file --> click Run Narrative Analysis (takes a few minutes)
- View results --> results are downloadable / saved in root directory data/sm_comments

---

# ğŸ“¥ 1. Data Collection

This system expects Facebook comments in JSON format.  
Each comment entry must include a `"text"` field.

Example raw item:

```json
{
  "id": "1913950356145953","typename": "SM_COMMENT","text": "I like turtles" .. ... ..... .......
}
```

---

# ğŸ“ 2. Repository Structure

Your project should follow this layout:

```
project_root/
â”‚
â”œâ”€â”€ ui.py                         # Streamlit interface
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ analysis.py               # Main narrative analysis logic
â”‚
â”œâ”€â”€ restructure-emoji.py          # Data normalization tool
â”‚
â”œâ”€â”€ # USE LOCAL LLM OR API KEY
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ social-media-comments.json    # NEED a "text" field
â”‚
â””â”€â”€ README.md
```

# ğŸ§¹ 3. Structure JSON

Before running the analysis, you must clean and standardize the JSON using: `restructure-emoji.py`


```

> âœ… This ensures maximum compatibility with the UI and backend.

```

# â–¶ï¸ 4. Running the Streamlit Interface

From the project root, run:

```bash
streamlit run ui.py
```

Streamlit will launch and show:

```
Local URL: http://localhost:8501
```

Autoopens / Open the link in your browser.

---

# ğŸ–¥ï¸ 5. Using the UI

### Step 1 â€” Choose Input
Select:

- **Paste text** (one comment per line)  
- **Upload JSON file**  

The UI automatically extracts valid comments.

### Step 2 â€” Preview
You'll see:

- Number of comments detected  
- First 10 comments for validation  

### Step 3 â€” Run Analysis
Click:

```
ğŸš€ Run Narrative Analysis
```

The backend produces:

- ğŸ§© Overall Summary  
- ğŸ“š Dominant Narratives  
- ğŸ’¬ Example Comments  
- ğŸ§¾ JSON Output (download available)

### Step 4 â€” Export
You can download:

```json
{
  "overall_summary": "...",
  "dominant_narratives": ["...", "..."],
  "example_comments": ["...", "..."]
}
```

---

# âš™ï¸ 6. Backend Processing Flow

Execution pipeline:

```
Raw Comments â†’ Clean â†’ Chunk â†’ LLM Summaries â†’ Merge â†’ Final Narratives JSON
```

### Internally, the backend:

1. Loads Qwen2.5-7B into GPU memory  
2. Splits comments into manageable chunks  
3. Generates summaries for each chunk  
4. Merges narratives into a single output  
5. Normalizes the output into:
   - One overall summary  
   - 3â€“6 dominant narratives  
   - 2â€“5 example comments  

> ğŸ§  Only clean structured JSON is exposed to the UI.

---

# ğŸ“¦ 7. Requirements

Install dependencies:

```bash
pip install streamlit transformers accelerate torch pydantic
```
